import collections
import tqdm

import TAM.utils as utils

def extract_advN(filename, source, file_id, accepted_nouns, output_directory):
	"""
	This Python function extracts and counts compound nouns, prefixes, and individual nouns from a given
	file, based on a list of accepted nouns, and saves the frequency counts in separate output files.

	Args:
	  filename: The `filename` parameter is the name of the file from which you are extracting
	adjectives and nouns.
	  source: Source refers to the origin or location of the file being processed. It could be a
	specific folder, database, or source system where the input file is stored.
	  file_id: The `file_id` parameter is a unique identifier or name for the file being processed. It
	helps in naming the output files generated by the function to distinguish them from other files
	processed by the same function.
	  accepted_nouns: Accepted_nouns is a list of nouns that have been approved or allowed for
	extraction. These are the nouns that meet certain criteria or conditions for being included in the
	extraction process.
	  output_directory: The `output_directory` parameter in the `extract_advN` function is the directory
	where the output files will be saved. It is the location where the generated files such as
	`{source}_{file_id}.compounds.tsv`, `{source}_{file_id}.prefs.tsv`, and `{source}_{file_id
	"""

	freqs = collections.defaultdict(int)

	prefs_freqs = collections.defaultdict(int)
	nouns_freqs = collections.defaultdict(int)

	for sentence in tqdm.tqdm(utils.read(filename, source)):
		for token in sentence.sentence:
			if token.pos == "NOUN" and "-" in token.form:
				formsplit = token.form.rsplit("-", 1)
				if formsplit[1] in accepted_nouns:
					freqs[token.form] += 1
					prefs_freqs[formsplit[0]] += 1
					nouns_freqs[formsplit[1]] += 1

	with open(output_directory.joinpath(f"{source}_{file_id}.compounds.tsv"), "w", encoding="utf-8") as fout:
		for key, f in freqs.items():
			print(f"{f}\t{key}", file=fout)

	with open(output_directory.joinpath(f"{source}_{file_id}.prefs.tsv"), "w", encoding="utf-8") as fout:
		for key, f in prefs_freqs.items():
			print(f"{f}\t{key}", file=fout)

	with open(output_directory.joinpath(f"{source}_{file_id}.nouns.tsv"), "w", encoding="utf-8") as fout:
		for key, f in nouns_freqs.items():
			print(f"{f}\t{key}", file=fout)


def extract_detADVN(filename, source, file_id, accepted_nouns, output_directory):
	"""
	This Python function extracts adverb-noun pairs from sentences based on specific patterns and saves
	the frequency of adverbs, nouns, and adverb-noun pairs to separate output files.

	Args:
	  filename: The `filename` parameter in the `extract_detADVN` function is the name of the file from
	which you are extracting data. It is the input file that contains the text data you want to analyze
	for specific patterns.
	  source: The `source` parameter in the `extract_detADVN` function is used to specify the source of
	the data being processed. It could be a file path, a data source identifier, or any other
	information that helps identify where the data is coming from. This parameter is important for
	organizing and categor
	  file_id: The `file_id` parameter is used to identify the specific file being processed. It helps
	in creating unique output filenames based on the source and file ID provided.
	  accepted_nouns: The `accepted_nouns` parameter in the `extract_detADVN` function is a list of
	nouns that are considered acceptable for extraction. These are the nouns that the function will
	focus on when processing the input data.
	  output_directory: The `output_directory` parameter in the `extract_detADVN` function is the
	directory where the output files will be saved. It is the location where the generated files such as
	`{source}_{file_id}.ngrams.tsv`, `{source}_{file_id}.adverbs.tsv`, and `{source}_{
	"""
	freqs = collections.defaultdict(int)

	adverbs_freqs = collections.defaultdict(int)
	nouns_freqs = collections.defaultdict(int)

	for sentence in tqdm.tqdm(utils.read(filename, source)):
		candidates = []

		for tok_id, token in enumerate(sentence.sentence):
			if token.pos == "ADV" and \
				tok_id > 2 and \
					tok_id < len(sentence.sentence)-2:
					candidates.append(tok_id)


		for c_id in candidates:

			adverb_object = sentence.sentence[c_id]

			pprevious_object = sentence.sentence[c_id-2]
			previous_object = sentence.sentence[c_id-1]

			next_object = sentence.sentence[c_id+1]
			nnext_object = sentence.sentence[c_id+2]

			if previous_object.pos == "DET":                                                # DET ADV ? ? case

				determiner_object = previous_object

				if (
					next_object.pos == "NOUN" and \
					((determiner_object.deprel == "" or determiner_object.head == adverb_object.id) or \
					(adverb_object.deprel == "" or adverb_object.head == next_object.id))
				):                                                                          # DET ADV NOUN case

					noun_object = next_object
					ngramtype = "DET ADV NOUN"


					if noun_object.form in accepted_nouns:
						freqs[(adverb_object.form, noun_object.form)] += 1
						adverbs_freqs[adverb_object.form] += 1
						nouns_freqs[noun_object.form] += 1
						# freqs[(determiner_object.form, adverb_object.form, noun_object.form)] += 1



				elif (
					next_object.pos == "ADV" and \
					nnext_object.pos == "NOUN" and \
					((determiner_object.deprel == "" or determiner_object.head == nnext_object.id) or \
					(adverb_object.deprel == "" or adverb_object.head == nnext_object.id) or \
					(next_object.deprel == "" or next_object.head == nnext_object.id))
				):                                                                          # DET ADV ADV NOUN case

					noun_object = nnext_object
					adverb_object.form = adverb_object.form + " " + next_object.form
					ngramtype = "DET ADV NOUN"


					if noun_object.form in accepted_nouns:
						freqs[(adverb_object.form, noun_object.form)] += 1
						adverbs_freqs[adverb_object.form] += 1
						nouns_freqs[noun_object.form] += 1
						# freqs[(determiner_object.form, adverb_object.form, noun_object.form)] += 1


				elif (
					next_object.pos == "ADJ" and \
					nnext_object.pos == "NOUN" and \
					((determiner_object.deprel == "" or determiner_object.head == nnext_object.id) or \
					(adverb_object.deprel == "" or adverb_object.head == nnext_object.id))
				):                                                                          # DET ADV ADJ NOUN case
					noun_object = nnext_object
					adj_object = next_object
					ngramtype = "DET ADV ADJ NOUN"


					if noun_object.form in accepted_nouns:
						freqs[(adverb_object.form, noun_object.form)] += 1
						adverbs_freqs[adverb_object.form] += 1
						nouns_freqs[noun_object.form] += 1
						# freqs[(determiner_object.form, adverb_object.form, adj_object.form, noun_object.form)] += 1


			if pprevious_object.pos == "DET":                                               # DET ? ADV ? case

				determiner_object = pprevious_object

				if (
					previous_object.pos == "ADJ" and \
					next_object.pos == "NOUN" and \
					((determiner_object.deprel == "" or determiner_object.head == next_object.id) or \
					(adverb_object.deprel == "" or adverb_object.head == next_object.id))
				):                                                                          # DET ADJ ADV NOUN case

					noun_object = next_object
					adj_object = previous_object
					ngramtype = "DET ADJ ADV NOUN"


					if noun_object.form in accepted_nouns:
						freqs[(adverb_object.form, noun_object.form)] += 1
						adverbs_freqs[adverb_object.form] += 1
						nouns_freqs[noun_object.form] += 1
						# freqs[(determiner_object.form, adj_object.form, adverb_object.form, noun_object.form)] += 1


	with open(output_directory.joinpath(f"{source}_{file_id}.ngrams.tsv"), "w", encoding="utf-8") as fout:
		for key, f in sorted(freqs.items(), key=lambda x: -x[1]):
			if f > 0:
				print(f"{f}\t{' '.join(key)}", file=fout)

	with open(output_directory.joinpath(f"{source}_{file_id}.adverbs.tsv"), "w", encoding="utf-8") as fout:
		for key, f in sorted(adverbs_freqs.items(), key=lambda x: -x[1]):
			if f > 0:
				print(f"{f}\t{key}", file=fout)

	with open(output_directory.joinpath(f"{source}_{file_id}.nouns.tsv"), "w", encoding="utf-8") as fout:
		for key, f in sorted(nouns_freqs.items(), key=lambda x: -x[1]):
			if f > 0:
				print(f"{f}\t{key}", file=fout)


def extract_NOUN(filename, source, file_id, output_directory):
	"""
	This Python function extracts and counts the frequency of nouns from a given file and writes the
	results to a TSV file in the specified output directory.

	Args:
	  filename: The `filename` parameter in the `extract_NOUN` function is typically the path to the
	file containing the text data from which you want to extract nouns. This file could be in various
	formats such as plain text, JSON, XML, etc. It is the source from which the function will read
	  source: The `source` parameter in the `extract_NOUN` function likely refers to the source of the
	data or text being processed. It could be a file, a database, or any other source from which the
	function is reading the sentences to extract nouns.
	  file_id: The `file_id` parameter in the `extract_NOUN` function is used to uniquely identify the
	output file that will be generated. It is typically a string or number that helps differentiate the
	output file for a specific source and processing run.
	  output_directory: The `output_directory` parameter in the `extract_NOUN` function is the directory
	where the output file will be saved. It is the location where the function will write the results of
	the noun extraction process.
	"""

	accepted_chars = "abcdefghijklmnopqrstuvwxyzàèéìòù"
	freqs = collections.defaultdict(int)

	for sentence in tqdm.tqdm(utils.read(filename, source)):
		for token in sentence.sentence:
			if token.pos == "NOUN":
				if all(c.lower() in accepted_chars or c in ["-", ".", " "] for c in token.form) and any(c not in ["-", ".", " "] for c in token.form):
					freqs[token.form] += 1

	with open(output_directory.joinpath(f"{source}_{file_id}.nouns.tsv"), "w", encoding="utf-8") as fout:
		for key, f in sorted(freqs.items()):
			print(f"{f}\t{key}", file=fout)


if __name__ == "__main__":
	accepted_nouns = utils.load_from_file("../data/output/REPUBBLICA_01.NOUNS.tsv", 2)

# TODO: extract nouns
# TODO: only consider base nouns more frequent than X
